Software Requirements Specification for {Project} 
===================================================
:Author: Mitch Griffith 
:Email: griffithmr1@ornl.gov
:Date: 2015-03-03
:Revision: 0.1
:Organization: National Center for Computational Sciences at the Oak Ridge +
National Laboratory
:Project: jitStage

This paper used the resources of the Oak Ridge Leadership Computing
Facility, located in the National Center for Computational Sciences at the
Oak Ridge National Laboratory, which is managed by UT Battelle, LLC for
the U.S. Department of Energy, under the contract No. DEAC05-00OR22725.

:numbered!:
Revision History
----------------
.Revision History
[options="header"]
|===========================================================================
|Name           | Date       | Reason For Change                   | Version
|Mitch Griffith | 2015-02-04 | initial version                     | 0.1
|===========================================================================

:numbered:
Introduction
------------
Background
~~~~~~~~~~
Over the years at the National Center for Computational Sciences (NCCS)
located at Oak Ridge National Laboratory (ORNL),
computing resource storage, or scratch area, has been limited in the
total amount of data that can be stored for immediate access.  HPSS
provides a solution to allow long-term archival storage space to users and
projects.  Archival storage space allows users to keep their
data longer and more reliably than on scratch areas.  Purge policies in
place on scratch areas require users to move data between HPSS and the
scratch areas to ensure the data is not lost.  With this requirement
users must, transfer data from HPSS to the scratch area(s), ensure the
data is not purged from the scratch area(s) until the job runs, once the
job is complete, the user needs to do _something_ with the data.
Typically, the user will archive the data to HPSS.

This process has made data analysis and simply running jobs difficult for
the user because:

* It is not feasible to move data during job execution because we are not
assured HPSS will be available (scheduled/unscheduled downtimes).
* We do not know how long the transfer will take (variablity in
network and resource contention).
* Resources may not be available (scratch area is full).

MOAB
^^^^
We currently run MOAB and Torque on all of our compute resources at NCCS.
MOAB 8 includes a new way of doing data staging on its compute resources.
We were able to install MOAB 8 on a test system.

In further review of the data staging feature, we discovered that msub was
required to take advantage of the new data staging features.  Msub
requires you submit to the MOAB scheduler directly instead of Torque.
This would require too much modification on behalf of all users since msub
is disabled at the NCCS to help simplify user interaction on the systems.

MOAB uses a file to indicate where to get the data, and requires the data
to be accessible without extra authentication.  This would work for us
with HPSS data since all users has passwordless access to HPSS by default.
It gets more difficult if we wanted to allow users to copy from file
systems that are not on the same machine, i.e. remote copy.  This would
require additional thought in how to authenticate the user to secure the
data properly.

MOAB Implementation Issues
++++++++++++++++++++++++++
MOAB does not provide a method to make a file/directory purge exempt.
There is no concept of puragable areas in MOAB.  This makes it difficult
to tie into job submission to allow needed data for a job to remain on the
scratch areas without the purge policy potentially erasing the data before
the job is complete.

It is also unclear in how scheduling is done.  MOAB allows you to set a
transfer size and bandwidth to the data store (in our case this is HPSS).
The issue we have is the resulting bandwidth may be more for large files
or less for smaller files.  It is also difficult to determine the time it
takes to transfer a file if it is on tape.  A user, or more likely a set
of users, could be using all tape drives and blocking the current
transfer.  Since there is no direct way to integrate with external
resources, it is not clear as to how to account for data that might be on
tape.  It is possible, but not likely, that it takes so long to transfer
the data that we hit the purge policy limit and data intended for job
execution is purged before the job ever uses it.  

We may also have periods of blocking utilization on the compute resources.
If MOAB schedules the job at 1:00 PM, and based on the size of the file
and bandwidth to the scratch area, MOAB determines it will take 1 hour to
transfer the file.

* MOAB will schedule the job to start at 1:00 PM.
* MOAB will start transferring the file at 12:00 PM.
* File actually takes 2 hours to transfer.
* The job does not start until 2:00 PM
* This can potentially block jobs that could have run in that hour from
starting 

We know we cannot block utilization based on what when we expect the
transfer to be complete.  We have sponsors that require a specific
utilization on the compute resources.  We just need a better way to manage
data resources to help ensure that we meet all requirements.


Purpose
~~~~~~~
We have a need to help users with getting analysis jobs pushed through
our analysis cluster while removing the burden of moving data from the user.
The user should be able to submit the job, and not have to worry if the
data will be available when the jobs runs.

{Project} is our proposed solution to the problem.

Scope
~~~~~
The {Project} is a client-server architecture implementation that will
help facilitate a user running an analysis job.  This project is limited
to Linux operating systems and x86_64 architectures.  The final product
should be available to all National Center for Computational
Sciences(NCCS) user facing systems.  

We will make any products developed for this purpose available to other
institutions as well.  As a result, we will need to adopt an appropriate
license for any software developed.  

Design Choices
~~~~~~~~~~~~~~
For our work at the NCCS, we have the option of doing several things.  For
instance:

* We can stage data in HPSS to the top level disk cache and 
create a purge lock for this file.  When the job is expected to start, we can
then transfer the file from HPSS to scratch.  This way we remove the time it
takes to transfer from tape to disk from problem.  Tape resources are not
required at this point.  
* Transfer data from HPSS to scratch and not worry about the purge lock in
HPSS.  This can cause issues about filling up scratch space.
* For scratch purging options, we can simply touch the files that we are
keeping track of instead of a general purge exception for the entire
directory.


Overall Description
-------------------
The idea behind {Project} is to implement a client/server interface that is 
capable of submitting _jobs_ to different resource managers like LSF and
PBS/torque as the end user.  The server will also be able to query jobs
and make policy based decisions based on those query results.

Product Perspective
~~~~~~~~~~~~~~~~~~~
The system will consist of 2 parts: the client and a server.  The server
runs on an indendant machine, and it is responsible for coordinating job 
executions, file retrievals, and storage purging. The client will communicate
with the server to submit jobs, check status of jobs, and termination of jobs.

The server will need to communicate with a database back-end.  All relevant
configuration, logs, server status, server configuration, purge locks,
and file system purge exemptions will be kept in this database back-end.
We will archive/purge older data to keep the overall size of the database
reasonable for the application.

The client will need to authenticate to the server in some way (yet to be
determinted) to authenticate the request.  This is needed because the
server shall have the ability to submit jobs to ORNL computing/data
resouces as the user.

Product Functions
~~~~~~~~~~~~~~~~~
With the client application, users will be able:

* submit jobs to {Project} 
* query job status
* delete their queued jobs

The server will handle coordinating with scratch space purging and archival
storage purge locks.  

We will need to work with the scratch filesystems to develop an API that will
allow custom handling of scratch and archival storage purge exemptions
and purge locks.

The server will be integrated with the standard resource manager to allow
the server to submit jobs to computing resources on the user's behalf.

Job Description
^^^^^^^^^^^^^^^
Jobs will consist of:

JobID::
    A unique job id associated with the current running {Project}
Username::
    The user that submitted the job
AccountID::
    The user's account that they want to associate with the job
DefaultGroup::
    The user's current default group when the job was submitted
SubmitTime::
    The time the job was submitted and recorded by {Project} server
TransferType::
    The type of transfer that is needed.  Get from HPSS.  Get from
GridFTP.
TransferSource::
    A list of files to transfer from the source
TransferDestination::
    A location to store the desired files
JobSubmitScript::
    The job submission script to submit to the compute resources

The remaining items are available only once a job completes:

EligibleTime::
    The time the job was made eligible to be scheduled
StartTime::
    The time the job was started by the {Project} server
EndTime::
    The time the job ended
CompletionCode::
    The status of the job when it completed.  Successful or failure.  This
should be a return code with a list of all possible values.


User Characteristics
~~~~~~~~~~~~~~~~~~~~
There are 3 types of users that interact with the system:

. end users
. storage coordinator APIs
. administrators

End users interaction will only use the client to submit, query, 
and delete jobs.  The user will have the ability to search all jobs and
query only their jobs.  

Administrators will not have any special ability in the client, but they
will have access to additional reporting and configuration commands on the
server machine.  The configuration commands will allow the administrator to 
configure the server with specific policies at the site location.  The
reporting commands will allow the administrator to view reports about
current status of the service and view reports relevant to the daily
operation of the service.

Storage coordinator APIs will allow external scripts/programs to be
written to help mitigate purging on scratch areas and setting purge locks
on the archival storage system.  The APIs will use a programmable
authentication mechanism to support scripting.

Constraints
~~~~~~~~~~~
The application is being developed for a Red Hat Enterprise Linux(RHEL)
version 6 X86_64 architecture.  We will do our best to ensure the servers
and client can run on other Linux operating systems, but in order to meet
our goals, we are limiting initial development to the infrastructure at
the NCCS.

The application will also be constrained by the capacity of the database.
It is possible to overload a database with a lot of queries that would
force queuing the incoming requests that would increase the amount of time
to fetch data.

The application will also use a MySQL database since this is what NCCS
supports at this time.

Operating Environment
~~~~~~~~~~~~~~~~~~~~~
The application will be developed for a RHEL 6 x86_64.  The application
should use the RHEL6 provided software to build and run.  
The application shall also require a MySQL database to hold configuration,
historical, and current state information.  

Design Implementation Constraints
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
{Product} shall be built to meet NCCS security requirements.  

* configurable to have encrypted communication between server and client
* configurable to have encrypted database connection
* be SE Linux compatible (limit the amount of requirements for SE Linux to
work correctly with the {Project}.

In order to fit into NCCS infrastructure, the application shall create
RPMs for easier integration into current configuration practices at NCCS.

User Documentation
~~~~~~~~~~~~~~~~~~
The application shall include:

* _man_ page for the client application that will list all available options
  and a brief description.
* Installation Guide - how to install the software and configure options
* Management Guide - how to manage the system and check status of the
application
* API Guide - how to use the API for scratch and archival storage purge
exemptions and locking

External Interface Requirements
-------------------------------

User Interfaces
~~~~~~~~~~~~~~~
A user will be able to use a command-line interface (CLI) to submit jobs.

Hardware Interfaces
~~~~~~~~~~~~~~~~~~~
The software shall be designed to run on an RHEL6 x86_64 machine.  The
software will not have any designated hardware, as a result it does not
have any direct hardware interfaces.  All hardware interfaces is expected
to be managed by the operating system.

Software Interfaces
~~~~~~~~~~~~~~~~~~~
The server application should communicate with the MySQL database using the
MySQL connector API provided by MySQL.  

Communications Interfaces
~~~~~~~~~~~~~~~~~~~~~~~~~
The API should use a Representational State Transfer (REST) API.  We have to
create this API from the server applciation.

The client application should communicate with the server via encrypted
SSL.

System Features
---------------

Client Application
~~~~~~~~~~~~~~~~

Submit a job
^^^^^^^^^^^^
TITLE: Users need to be able to submit jobs to {Project} +
DESC: A user submits their data queuing job and compute job to {Project} +
RAT: The user needs a way to submit requests to {Project}.  This is the
mechanism +
DEP: None +

Query a job
^^^^^^^^^^^
TITLE: Query a Job
DESC: A user can query a job that belongs to them.  
System Feature 2
~~~~~~~~~~~~~~~~
TITLE: +
DESC: +
RAT: +
DEP: +

Other Nonfunctional Requirements
--------------------------------

Performance Requirements
~~~~~~~~~~~~~~~~~~~~~~~~
The system should respond reasonably well under load.

Safety Requirements
~~~~~~~~~~~~~~~~~~~

Security Requirements
~~~~~~~~~~~~~~~~~~~~~
The system shall not expose any underlying strucutre to the client so we can
limit use.

:numbered!:
[appendix]
Glossary
--------

[glossary]
NCCS::
  National Center for Computational Sciences

OLCF::
  Oak Ridge Leadership Computing Facility

ORNL::
  Oak Ridge National Laboratory

RHEL::
  Red Hat Enterprise Linux


[appendix]
Acknowledgments
---------------
Sections of this paper are based upon the IEEE Guide to Software
Requirements Specification (ANSI/IEEE Std. 830-1984).

This paper used the resources of the Oak Ridge Leadership Computing
Facility, located in the National Center for Computational Sciences at the
Oak Ridge National Laboratory, which is managed by UT Battelle, LLC for
the U.S. Department of Energy, under the contract No. DEAC05-00OR22725.

ifdef::backend-docbook[]
[index]
Example Index
-------------
////////////////////////////////////////////////////////////////
The index is normally left completely empty, it's contents being
generated automatically by the DocBook toolchain.
////////////////////////////////////////////////////////////////
endif::backend-docbook[]
